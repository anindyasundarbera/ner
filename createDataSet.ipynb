{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9c5661-673e-46e8-813e-7660fae8b67c",
   "metadata": {},
   "source": [
    "## Generate the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd9ad80-3df6-403d-b627-ecc527e83502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1b08d7-565a-4dce-921b-8b785e657a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists of values\n",
    "verbs = [\"create\", \"add\", \"organize\", \"schedule\", \"set up\", \"plan\", \"book\", \"reserve\", \"arrange\", \"fix\", \"hold\"]\n",
    "relative_dates = [\n",
    "    \"next Monday\", \"tomorrow\", \"next Friday\", \"first Monday of July\", \"2nd August\", \"this Saturday\", \n",
    "    \"next Sunday\", \"next weekend\", \"weekend evening\", \"this Friday\", \"coming weekend\", \"next month\", \n",
    "    \"the day after tomorrow\", \"this Wednesday\", \"next Thursday\", \"next year\", \"this afternoon\", \n",
    "    \"next morning\", \"coming Friday\", \"this evening\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2133943b-c747-4361-b03a-427ecf889e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a future weekend date\n",
    "def get_future_weekend_dates():\n",
    "    weekend_dates = []\n",
    "    today = datetime.now()\n",
    "    for i in range(30):  # Generate dates for the next 30 days\n",
    "        future_date = today + timedelta(days=i)\n",
    "        if future_date.weekday() >= 5:  # Saturday (5) or Sunday (6)\n",
    "            weekend_dates.append(future_date.strftime(\"%Y-%m-%d\"))\n",
    "    return weekend_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb814f5d-4292-4d9e-bb42-ae2e0bd497ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of specific dates including weekends\n",
    "specific_dates = [(datetime.now() + timedelta(days=random.randint(1, 30))).strftime(\"%Y-%m-%d\") for _ in range(10)] + get_future_weekend_dates()\n",
    "times = [\n",
    "    \"09:00\", \"10:00\", \"afternoon\", \"09:00 AM\", \"evening\", \"morning\", \"14:00\", \"12:00\", \"weekend evening\", \n",
    "    \"07:30\", \"18:45\", \"midnight\", \"noon\", \"early morning\", \"late evening\", \"03:15 PM\", \"10:30 PM\", \n",
    "    \"04:00\", \"21:00\", \"11:00 PM\", \"02:00 AM\", \"08:45\", \"16:20\", \"06:00\", \"23:59\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30c34225-b832-4367-83b9-c0c3b8f29e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\"Afghanistan\",\"Albania\",\"Algeria\",\"Andorra\",\"Angola\",\"Anguilla\",\"Antigua &amp; Barbuda\",\"Argentina\",\"Armenia\",\"Aruba\",\"Australia\",\"Austria\",\"Azerbaijan\",\"Bahamas\",\"Bahrain\",\"Bangladesh\",\"Barbados\",\"Belarus\",\"Belgium\",\"Belize\",\"Benin\",\"Bermuda\",\"Bhutan\",\"Bolivia\",\"Bosnia &amp; Herzegovina\",\"Botswana\",\"Brazil\",\"British Virgin Islands\",\"Brunei\",\"Bulgaria\",\"Burkina Faso\",\"Burundi\",\"Cambodia\",\"Cameroon\",\"Cape Verde\",\"Cayman Islands\",\"Chad\",\"Chile\",\"China\",\"Colombia\",\"Congo\",\"Cook Islands\",\"Costa Rica\",\"Cote D Ivoire\",\"Croatia\",\"Cruise Ship\",\"Cuba\",\"Cyprus\",\"Czech Republic\",\"Denmark\",\"Djibouti\",\"Dominica\",\"Dominican Republic\",\"Ecuador\",\"Egypt\",\"El Salvador\",\"Equatorial Guinea\",\"Estonia\",\"Ethiopia\",\"Falkland Islands\",\"Faroe Islands\",\"Fiji\",\"Finland\",\"France\",\"French Polynesia\",\"French West Indies\",\"Gabon\",\"Gambia\",\"Georgia\",\"Germany\",\"Ghana\",\"Gibraltar\",\"Greece\",\"Greenland\",\"Grenada\",\"Guam\",\"Guatemala\",\"Guernsey\",\"Guinea\",\"Guinea Bissau\",\"Guyana\",\"Haiti\",\"Honduras\",\"Hong Kong\",\"Hungary\",\"Iceland\",\"India\",\"Indonesia\",\"Iran\",\"Iraq\",\"Ireland\",\"Isle of Man\",\"Israel\",\"Italy\",\"Jamaica\",\"Japan\",\"Jersey\",\"Jordan\",\"Kazakhstan\",\"Kenya\",\"Kuwait\",\"Kyrgyz Republic\",\"Laos\",\"Latvia\",\"Lebanon\",\"Lesotho\",\"Liberia\",\"Libya\",\"Liechtenstein\",\"Lithuania\",\"Luxembourg\",\"Macau\",\"Macedonia\",\"Madagascar\",\"Malawi\",\"Malaysia\",\"Maldives\",\"Mali\",\"Malta\",\"Mauritania\",\"Mauritius\",\"Mexico\",\"Moldova\",\"Monaco\",\"Mongolia\",\"Montenegro\",\"Montserrat\",\"Morocco\",\"Mozambique\",\"Namibia\",\"Nepal\",\"Netherlands\",\"Netherlands Antilles\",\"New Caledonia\",\"New Zealand\",\"Nicaragua\",\"Niger\",\"Nigeria\",\"Norway\",\"Oman\",\"Pakistan\",\"Palestine\",\"Panama\",\"Papua New Guinea\",\"Paraguay\",\"Peru\",\"Philippines\",\"Poland\",\"Portugal\",\"Puerto Rico\",\"Qatar\",\"Reunion\",\"Romania\",\"Russia\",\"Rwanda\",\"Saint Pierre &amp; Miquelon\",\"Samoa\",\"San Marino\",\"Satellite\",\"Saudi Arabia\",\"Senegal\",\"Serbia\",\"Seychelles\",\"Sierra Leone\",\"Singapore\",\"Slovakia\",\"Slovenia\",\"South Africa\",\"South Korea\",\"Spain\",\"Sri Lanka\",\"St Kitts &amp; Nevis\",\"St Lucia\",\"St Vincent\",\"St. Lucia\",\"Sudan\",\"Suriname\",\"Swaziland\",\"Sweden\",\"Switzerland\",\"Syria\",\"Taiwan\",\"Tajikistan\",\"Tanzania\",\"Thailand\",\"Timor L'Este\",\"Togo\",\"Tonga\",\"Trinidad &amp; Tobago\",\"Tunisia\",\"Turkey\",\"Turkmenistan\",\"Turks &amp; Caicos\",\"Uganda\",\"Ukraine\",\"United Arab Emirates\",\"United Kingdom\",\"Uruguay\",\"Uzbekistan\",\"Venezuela\",\"Vietnam\",\"Virgin Islands (US)\",\"Yemen\",\"Zambia\",\"Zimbabwe\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23441ba1-2bc9-40fe-b2cd-1ff7b5f8d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "# Generate rows with relative and specific dates, including weekends\n",
    "for _ in range(5000):  # Number of rows in the dataset\n",
    "    row = [\n",
    "        random.choice(verbs),\n",
    "        random.choice(relative_dates + specific_dates),\n",
    "        random.choice(times),\n",
    "        random.choice(countries)\n",
    "    ]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f3ae84-738e-4435-8910-1c6193be2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "with open('event_data.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Verb\", \"Date\", \"Time\", \"Country\"])\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f290ba-f419-4cfc-876d-3fa7f4b5eff1",
   "metadata": {},
   "source": [
    "Read and Parse the CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc91944-f4b1-410e-bfd4-7c640bd37d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e10bbf-1c0b-4874-89a2-98701d3cc431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb5e18-c7f0-4c97-b24c-7d4bb036a270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba55f16-1713-4cd1-bdfd-54963cd069a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083abefd-652a-4fb6-8d40-1f11aa7e74ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f13911b9-4211-4b1f-8589-c916755d7f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "from spacy.training import offsets_to_biluo_tags, Example\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94bee1d9-fc44-4a77-884a-920f9038401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "with open('event_data.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header\n",
    "    rows = [row for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b470eabe-b853-4432-8e7f-5425835b4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def has_overlap(entity1, entity2):\n",
    "    return not (entity1[1] <= entity2[0] or entity2[1] <= entity1[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "598e8beb-505c-46a6-a9ca-28cd5b3ec335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping overlapping entities in: hold an event on weekend evening at weekend evening in St Vincent\n",
      "Skipping overlapping entities in: fix an event on weekend evening at evening in Bulgaria\n"
     ]
    }
   ],
   "source": [
    "# Annotate and check for overlapping entities\n",
    "annotated_data = []\n",
    "\n",
    "for row in rows:\n",
    "    verb, date, time, country = row\n",
    "    sentence = f\"{verb} an event on {date} at {time} in {country}\"\n",
    "    start_date = sentence.find(date)\n",
    "    end_date = start_date + len(date)\n",
    "    start_time = sentence.find(time)\n",
    "    end_time = start_time + len(time)\n",
    "    \n",
    "    entities = [(start_date, end_date, \"DATE\"), (start_time, end_time, \"TIME\")]\n",
    "    \n",
    "    # Check for overlaps\n",
    "    valid_entities = True\n",
    "    for i, ent1 in enumerate(entities):\n",
    "        for ent2 in entities[i+1:]:\n",
    "            if has_overlap(ent1, ent2):\n",
    "                valid_entities = False\n",
    "                break\n",
    "        if not valid_entities:\n",
    "            break\n",
    "    \n",
    "    if not valid_entities:\n",
    "        print(f\"Skipping overlapping entities in: {sentence}\")\n",
    "        continue\n",
    "    \n",
    "    annotated_data.append((sentence, entities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96b62faa-eab6-41d7-8e2c-533046e08d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate with BILUO tags and filter out misaligned entities\n",
    "aligned_data = []\n",
    "\n",
    "for sentence, entities in annotated_data:\n",
    "    doc = nlp.make_doc(sentence)\n",
    "    tags = offsets_to_biluo_tags(doc, entities)\n",
    "    \n",
    "    if '-' in tags:\n",
    "        print(f\"Skipping misaligned entities in: {sentence}\")\n",
    "        continue\n",
    "    \n",
    "    aligned_data.append((sentence, {\"entities\": [(start, end, label) for start, end, label in entities]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c41c2e09-86d5-401e-a700-79fbbd97db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DocBin to hold the examples\n",
    "doc_bin = DocBin(attrs=[\"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27ef9a47-c89a-4d9c-b9de-0ac9a0cce025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to spaCy Examples\n",
    "for text, annotations in aligned_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    entities = annotations[\"entities\"]\n",
    "    \n",
    "    # Filter out any None spans and add to doc.ents\n",
    "    spans = [doc.char_span(start, end, label=label) for start, end, label in entities]\n",
    "    spans = [span for span in spans if span is not None]\n",
    "    filtered_spans = filter_spans(spans)\n",
    "    \n",
    "    doc.ents = filtered_spans\n",
    "    example = Example.from_dict(doc, {\"entities\": [(span.start_char, span.end_char, span.label_) for span in filtered_spans]})\n",
    "    doc_bin.add(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5990431-3e3c-4a5b-b4ce-941f9295afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DocBin to disk\n",
    "doc_bin.to_disk(\"train_data.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d41803b-06a9-40ab-ad46-08cb52b3ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a blank model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add the NER pipeline component\n",
    "ner = nlp.add_pipe(\"ner\", last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9878af0e-c192-4e41-849a-545df9a1b0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add labels\n",
    "ner.add_label(\"DATE\")\n",
    "ner.add_label(\"TIME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0bb46e5-aa7c-4c07-bbd7-ac494511477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DocBin\n",
    "doc_bin = DocBin().from_disk(\"train_data.spacy\")\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53fa7f10-d676-4e4c-ab73-a29c07829bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Examples from the Docs\n",
    "examples = [Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]}) for doc in docs]\n",
    "# print(examples)\n",
    "subset_examples = examples[:50]\n",
    "# print(subset_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df69ee49-15a2-486f-8dd8-26209b1a11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbe81c0d-6909-4be2-ac3a-a0888cde7345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: {'ner': 1325.7439578518347}\n",
      "Iteration 1, Loss: {'ner': 32.80667736957226}\n",
      "Iteration 2, Loss: {'ner': 7.950145536365379}\n",
      "Iteration 3, Loss: {'ner': 10.956873692909708}\n",
      "Iteration 4, Loss: {'ner': 13.37251582275271}\n",
      "Iteration 5, Loss: {'ner': 14.134979961108288}\n",
      "Iteration 6, Loss: {'ner': 5.351147739555626}\n",
      "Iteration 7, Loss: {'ner': 4.650054430943933}\n",
      "Iteration 8, Loss: {'ner': 0.009476099322786553}\n",
      "Iteration 9, Loss: {'ner': 0.3401773981752194}\n",
      "Iteration 10, Loss: {'ner': 0.0005184109806227835}\n",
      "Iteration 11, Loss: {'ner': 5.927365525646753}\n",
      "Iteration 12, Loss: {'ner': 18.236063249840424}\n",
      "Iteration 13, Loss: {'ner': 12.005968177637772}\n",
      "Iteration 14, Loss: {'ner': 10.703453215710304}\n",
      "Iteration 15, Loss: {'ner': 18.69195145987872}\n",
      "Iteration 16, Loss: {'ner': 7.797556806981088}\n",
      "Iteration 17, Loss: {'ner': 11.687306719701766}\n",
      "Iteration 18, Loss: {'ner': 6.048206476929498}\n",
      "Iteration 19, Loss: {'ner': 4.130986206203999e-05}\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "for itn in range(20):  # Number of iterations\n",
    "    random.shuffle(examples)\n",
    "    losses = {}\n",
    "    batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
    "    # batches = minibatch(subset_examples, size=4)\n",
    "    for batch in batches:\n",
    "        nlp.update(batch, drop=0.5, losses=losses)\n",
    "    print(f\"Iteration {itn}, Loss: {losses}\")\n",
    "\n",
    "# # Reduce the number of training iterations\n",
    "# for itn in range(10):  # Lower the number of iterations\n",
    "#     random.shuffle(subset_examples)\n",
    "#     losses = {}\n",
    "#     batches = minibatch(subset_examples, size=compounding(2.0, 16.0, 1.001))\n",
    "#     for batch in batches:\n",
    "#         nlp.update(batch, drop=0.5, losses=losses)\n",
    "#     print(f\"Iteration {itn}, Loss: {losses}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be7b28ec-1387-42bd-8509-279b44e85f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "nlp.to_disk(\"event_date_time_model_with_alignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6876a2-66a4-4a41-9835-e78564ea1c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
